# ðŸ§  Perceptual Compression Pipeline: Engineering Specification

---

## ðŸ§® 1. Scene Complexity Estimation

To dynamically allocate encoding depth:

Let  
- `E_d` = Edge density score  
- `O_c` = Object count  
- `C_v` = Color variance  
- `W âˆˆ â„` = Weighted complexity score

Then:

    W = Î±â‚Â·E_d + Î±â‚‚Â·O_c + Î±â‚ƒÂ·C_v

Where `Î±â‚, Î±â‚‚, Î±â‚ƒ` are context-dependent scalars.

Encoding length `L âˆˆ {12, 16, 26, 32}` is selected by thresholding `W`.

---

## ðŸ”„ 2. Adaptive Symbol Allocation (Encoder Core)

Each character in the perceptual code represents a **symbolic feature vector**, drawn from:

    Î£ = {A-Z}
    Î£â€² = Î£ âˆª {#, *, ?, 0-9}

Each position `páµ¢` in the code:

    páµ¢ = f(Váµ¢, Sáµ¢, Máµ¢)

Where:  
- `Váµ¢`: Visual saliency at region `i`  
- `Sáµ¢`: Spatial quadrant tag  
- `Máµ¢`: Mood/affective weighting  
- `f`: Symbol mapping function

---

## â± 3. Temporal Contextualization (Video Mode)

To improve encoding consistency across frames:

### Frame Delta Representation

For frame `F_t`, compute:

    Î”_t = F_t - F_{t-1}

Encode:
- `K_t`: Keyframe â†’ full symbolic encoding  
- `D_t`: Delta frame â†’ compact 8â€“12 character update  
- `M_t`: Motion vector â†’  
    [CameraCode | Object1 | Object2 | Object3]

Camera codes `C âˆˆ {00â€“09}`  
Object tracking uses relative velocity vectors discretized into 2-character bins.

---

## ðŸ” 4. Cultural & Personal Vocabulary Adaptation

Each user has a dictionary `D_u`:

    D_u = D_base âˆª D_personal âˆª D_cultural

Encoder selects tokens from `D_u` by emotional salience and familiarity.

Learning function:

    D_uâ€² = ð“›(D_u, I_accepted, I_rejected)

---

## ðŸ“ 5. Multi-Scale Hierarchical Encoding

Three perceptual layers:
1. Global `G` â€“ 26 chars  
2. Intermediate `I` â€“ 13 chars  
3. Fine `F` â€“ 13 chars  

Combined encoding:

    C = G âŠ• I âŠ• F

---

## ðŸ“Š 6. Decoder Symbol Expansion

Given symbolic code `S âˆˆ Î£^L`:

    Scene_Model = ð““(S) = {O, M, Q, T}

Where:  
- `O`: Objects  
- `M`: Mood  
- `Q`: Focus quadrant  
- `T`: Temporal echoes

Decoder uses:
- Symbol-to-concept maps  
- Mood-spatial graph builder  
- Optional randomness for NPC/â€œdreamlikeâ€ mode

---

## âš ï¸ 7. Disambiguation System

Multiple interpretations `Sáµ¢`:

    score(Sáµ¢) = sim(Sáµ¢, C_s)

Use context stack `C_s` and scoring function to resolve conflicts.

---

## ðŸ” 8. Versioned Encoding Format

Format:

    Code = V_v âŠ• S_d

Where:
- `V_v`: Version tag (2 chars)  
- `S_d`: Scene symbol stream

Version registry ensures backward compatibility.

---

## ðŸ“‰ 9. Compression Metrics

Estimates:

- ~2.6KB per minute  
- ~90MB/min raw video (720p)  
- Ratio:

    CR â‰ˆ 90MB / 2.6KB â‰ˆ 34,615:1

Semantic density > visual fidelity

---

## âœ… Summary

This pipeline enables:

- Ultra-low-bandwidth memory replay  
- NPC perception with emotional weight  
- Symbolic scene logging  
- Shared understanding of visually perceived environments

Meaning first. Pixels later.
